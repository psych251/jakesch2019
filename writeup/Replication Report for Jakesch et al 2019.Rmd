---
title: "Replication of Experiment 3 by Jakesch et. al. (2019, ACM CHI Conference on Human Factors in Computing Systems)"
author: "Jacob Ritchie (jritchie@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

In the ACM CHI 2019 paper "AI-Mediated Communication: How the Perception that Profile Text was Written by AI Affects Trustworthiness"  (Jakesch et. al. 2019), the authors conducted a Wizard-of-Oz crowdworker study to test the hypothesis that users would react to text from an online marketplace listing differently when they believed this text had been generated automatically using "artificial intelligence" (AI) ^[Otherwise known as machine learning or statistical function approximation.]. In particular, they investigated whether the belief that an AirBnB host profile was written by AI would decrease the perceived trustworthiness of the host described in the profile. They found evidence for what they called the "Replicant Effect": that the belief that a profile was written by AI decreased perceived trustworthiness, but *only* for the case where participants believed that some, but not all profiles were written by AI.
 
In this replication, we focus on independently confirming one part of the Replicant Effect - that when presented with a mixed set containing both "human-generated" and "AI-generated" profiles, participants will assign lower trustworthiness to those profiles that seem the most "AI-like". We do this by replicating Experiment 3 from the original paper, which found that "AI-like" papers were given significantly significantly lower trustworthiness scores by participants in three treatment conditions where they were told that a subset of the profiles they were rating were AI-generated^[In reality, all of the profiles were human-generated.]. We focus on the "primed" treatment condition of Experiment 3, which resulted in the largest observed effect size  In this treatment, participants are asked to rate how likely they believed it was that each profile had been written by AI before they assigned it a trustworthiness rating.

We leave to future work a replication of the other half of the Replicant Effect - that when participants are presented with a sample of profiles and told that all the profiles are AI-generated, then they assign the same trustworthiness ratings as when they believe that all the profiles are human-written.

### Links

See http://rpubs.com/jritch/jakesch2019_proposal for the original project proposal.

See http://rpubs.com/jritch/jakesch2019 for the most recent project report.

See http://github.com/psych251/jakesch2019 for the complete GitHub project.

See https://stanforduniversity.qualtrics.com/jfe/form/SV_5uM01U1dpW8KJDv for the Qualtrics survey.

See https://workersandbox.mturk.com/requesters/A38O2BV2MPHCQD/projects?ref=w_pl_prvw for the Amazon Mechanical Turk listing.

## Methods

### Power Analysis

To be conducted on Monday Oct 28 after power analysis is covered in class.

The original paper reports a large effect size in Experiment 3 (total R^2^ = 0.48), so we anticipate that a power analysis will show that smaller sample is needed than in the original study.

In terms of feasibility, the original experiment required 323 participants working for six minutes each, resulting in 32.3 person-hours of work in total, which is not feasible given the budget available for the project. This is why this replication focuses on only one of the three treatment conditions.

### Planned Sample

Based on the power analysis, we planned to recruit [*TO BE DETERMINED*] participants.

As in the original paper, we recruited participants from Amazon Mechanical Turk. We used the participation criteria from the original paper:

> "Participation was limited to adults in the US who had completed at least 500 tasks with an approval rate of >= 98%."

### Materials

We used the materials described in the original paper.

To explain the Wizard-of-Oz AI system to the participants in the treatment condition, we used the explanation and demo video from the original experiment:

>"In the final design, we explained the AI system as follows: 'To help hosts create profiles that are more attractive, this site provides a computer system using artificial intelligence that will write the description for hosts. The hosts simply enter some information and the artificial intelligence system generates the profile.' The participants in the treatment group then watched a 10-second demo video of a mock-up AI system... In the video, an animation depicts a system automatically generating text for an Airbnb profile from a Facebook profile"

We used the three-item trustworthiness rating scale created for the original experiment (each item ranged from 0-100):

>"1. This person maintains a clean, safe, and comfortable household.
2. This person will be concerned about satisfying my needs during the stay.
3. This person will not intentionally harm, overcharge, or scam me."

And also used the generalized trust and AI attitude scales that were used in the original experiment. In the treatment condition, we used the AI score measurement described in the original study:

> '...respondents indicated whether they thought each profile was (1) "Definitely Human-written" to (6) "Definitely AI-generated" on a 6-point Likert-style scale. We refer to this measurement as the "AI score" of a profile." '

We used the original set of 30 sample profiles from the original paper's Experiment 3, which we downloaded from the project's OSF repository. These had been determined in a previous mTurk task to be the 15 most "AI-like" and 15 most "human-like" from a larger sample of profiles, using the AI score measurement described above (see Table 1 for an example).

A variety of other questions were also asked, as in the original study:

> "Participants also answered demographic questions (gender, age, education, and residential neighborhood type), as well as free-form questions explaining how they rated the profiles. We finally asked what they thought was the purpose of this study, and, in the treatment group, whether they had comments on the system."

We contacted the authors to request a copy of the exact questions and instructions given to the participants (which were not all included in the authors' publically available OSF repository), including the video prompt. This ensured that the same questions were used, and that the framing and context of each question was as close as possible to the original experiment.

Note - I found that many of the question prompts were available in the authors "data_study3.csv" file. However, I still need to contact them to get the few question wordings that are not there.

```{r, include=FALSE}
library(stats)
library(nlme)
library(dplyr)
library(kableExtra)
library(tidyr)
library(stringr) # useful for some string manipulation
library(ggplot2)
library(plotrix)
```


```{r, echo=FALSE}
reviews <- tibble("AI Score"=c("5 (most AI-like)", "1.75 (most human-like)"), "Profile Text"=c("Hey, I'm an health nut and entrepreneur. Enjoy cooking, reading, writing. Favorite restaurant nearby: Momofuku Ssam bar. Favorite weird activity nearby: Russian Baths on E. 10th St. Heaven is three grocery stores and a farmer's market within two blocks (Trader Joe's, Whole Foods, Westside Market, farmer's market", "I am a musician that travels with my band a lot. I have a place in Hawaii as well that I spend time in. I'm clean. funky. and a fun artist. Love yoga, books, writing, singing, dancing, dj'ing, art, and the list goes onnn. "))
knitr::kable(reviews,caption="Table 1: The top 'human-like' and 'AI-like' sample profiles used in Experiment 3.") %>%  column_spec(1, width="12em")
```


### Procedure

We followed the procedure from the original paper exactly (except that we only had one treatment group, instead of three):

>"We randomly assigned participants to one of four groups [*note: two in the replication study*]: The control group participants were told they were rating regular profiles written by the host (akin to the "host-written" group in Study 1). In the treatment groups, participants were told that "some of the profiles [they] see have been generated by a computer system using artificial intelligence, while others have been written by the host." Treatment group participants also viewed the [demo video]."

> "We performed a manipulation check to verify that the participants in the treatment condition understood that a profile had been generated."

After the introduction to the experiment, participants then completed the trustworthiness rating questions (see Materials):  

> "Participants rated 10 profiles in randomized order: five 'AI' profiles (out of the 15 profiles rated as AI-like in the preliminary selection) and five 'human' profiles (out of the 15 profiles rated human-like)..."

> "Subjects in the primed condition were not shown any labels, but we asked them... to rate the AI score of a profile before they rated the host's trustworthiness."

After they rated all ten profiles, participants answered the generalized trust and AI attitude scales, and answered the demographic and open-ended questions from the original study.

### Planned Analysis 

We followed the exclusion criteria from the original paper:

> "We excluded responses that had failed the linguistic attentiveness check borrowed from Munro et al. [43] as well as participants who did not select the right scenario ("I am traveling and the person in the profile offers to host me.") in a second attentiveness test. We excluded workers whose median rating time per profile was less than five seconds and workers with mostly uniform responses (SD < 5.0). Furthermore, we removed participants whose average trust rating fell outside the mean +/- 2 SD statistic of participant rating averages, leaving us with 389 subjects."

We contacted the authors to ask for the exact wording of their attentiveness tests (since it is not clear what exactly the scenario description given for the second attentiveness test was).

The mean of the three items in the trustworthiness scale was averaged to create an overall trustworthiness rating. (We first calculated Cronbach's alpha for the three items to validate that this was acceptable).

Then, we fit a generalized linear mixed model (GLMM) with random per-participant effects to each participants' trustworthiness ratings: `lme(meanScore ~ ai * condition, random=~ 1|ResponseId)`.

### Differences from original study

We used a lower rate of compensation than the original study (7.25 USD/hour in the replication compared to 12 USD/hour in the original study), which may have resulted in quicker and less thoroughly considered answers if the crowd workers compensated for the lower rate of pay by completing the task more quickly. However, we don't expect that this significantly impacted the results.

Because of the limited resources available for the replication, we only replicated one of the three treatment conditions ("labeled", "unlabeled" and "primed') from the original paper (the "primed" condition). However, since the three treatment conditions produced similar results in the original study, and the 'primed' treatment condition resulted in the largest effect size, we believe that testing the replicability of this condition is sufficient.

During analysis of the pilot data I realized that there were some issues with the randomization of the different conditions. In the original experiment, participants saw 5 of the 15 "AI-generated" profiles and 5 of the 15 "human-generated" profiles, in a random order. The way I currently have the Qualtrics survey set up, the participants see the ten profiles in two blocks each (5 of the 15 "human" profiles and 5 of the 15 "AI" profiles). The order within these blocks is randomized, as is the order of the blocks, but the "human" and "AI" profiles are not mixed together as they were in the original. I will fix this error in randomization ahead of pilot B, since it might otherwise create an experimental confound. This might result in me changing some of my code for processing the raw Qualtrics data, but the rest of the code should be unchanged.

### Methods Addendum (Post Data Collection)
**TODO**

#### Actual Sample
**TODO**

#### Differences from pre-data collection methods plan
**TODO**

### Results

## Data Preparation

We excluded responses that met the exclusion criteria described in the original paper. 

For each profile, the mean of the three items in the trustworthiness scale was calculated and used as an overall trustworthiness rating, as done in the original study.


```{r}
responses <- read.csv(file="pilot_data_8_responses.csv") %>% 
  # filter out the Qualtrics header info lines
  # select only the columns needed for the confirmatory analysis
  filter(!between(row_number(),1,2)) %>% 
  # filter out participants who did not consent 
  # or who revoked consent after the debrief
  # or who failed one of the two attention check questions
  filter(
    Consent == 'I CONSENT to take part in the study.',
    debrief_agree == "I AGREE that the data collected during the study may be included for the purpose of the study.",
    att_check_1 == "John is easy to see.",
    att_check_2 == "I am traveling and the person in the profile offers to host me.",
    (ai_comprehension == "Some were generated by the AI system, some were written by the host." | ai_comprehension == "")
  ) %>%
  select(starts_with("X"), -contains("ai_score"), c("ResponseId"))

long_responses <- responses %>% 
  gather (measurement, score, -c("ResponseId")) %>% 
  mutate(
    score = as.numeric(score),
    measurement = gsub('.{2}$', '', measurement), 
    # ai is TRUE if the profile is "ai-generated, 
    # and FALSE if the profile is "human-generated"
    ai = grepl("ai", measurement), 
    # condition is FALSE if the participant is in the control condition
    # and TRUE if they are in the primed condition
    condition = grepl("primed",measurement),
    # this assigns each response a profile number that corresponds
    # to the numbers from the original experiment's csv file
    # profiles 0-15 are "AI", 16-30 are "human"
    profile_number = as.numeric(sub("X([0-9]*)_.*","\\1",measurement)) 
                    + 15 * (1 - as.numeric(ai)), 
    # which of the three items on the 3-item trustworthiness scale is this? 
    item_number = as.numeric(str_sub(measurement,-1,-1))
  )

long_responses_grouped <- long_responses %>%
  group_by(ResponseId,profile_number,condition,ai) %>%
  summarize(meanScore = mean(score, na.rm = T)) %>% 
  filter(!is.na(meanScore)) %>% 
  ungroup()

# for each participant, calculate their average trustworthiness rating
# and the std. deviation of their ratings
participant_average_rating = long_responses_grouped %>% group_by(ResponseId) %>% summarize(meanRating = mean(meanScore),
          sdRating = sd(meanScore))
# calculate the mean std. deviation and mean 
# of ALL per-participant average trustworthiness ratings
rating_mean = mean(participant_average_rating$meanRating)
rating_sd = sd(participant_average_rating$meanRating)

# get list of participants to exclude
# based on either overly uniform scores, 
# or on an extreme average score
participants_to_exclude = participant_average_rating %>% filter((meanRating > rating_mean + 2 * rating_sd) | meanRating < (rating_mean - 2 * rating_sd) | (sdRating < 5.0) ) %>% select(ResponseId)

# for participants in the control condition, we only care about the "Page.Submit"
# value of each rating question page
control_timing_data <- responses %>% gather(time_interval, time, contains("timing_"), -contains("Click")) %>% select(time_interval,time,ResponseId) %>% group_by(ResponseId) %>% summarize(medianRatingTime = median(as.numeric(time),na.rm=T)) %>% filter(medianRatingTime > 0)

# For participants in the primed condition, we have to add up the values for the time 
# They spend on the AI Score rating and the Trustworthiness rating
primed_timing_data <- responses %>% 
  gather(time_interval, time, contains("timing1"), contains("timing2"), -contains("Click")) %>% 
  mutate(profile_number = as.numeric(sub("X([0-9]*)_.*","\\1",time_interval))) %>%
  select(time_interval,time,ResponseId,profile_number) %>% group_by(ResponseId,profile_number) %>%  
  summarize(time = sum(as.numeric(time), na.rm = T)) %>% 
  ungroup() %>% group_by(ResponseId) %>% 
  summarize(medianRatingTime = median(as.numeric(time),na.rm=T)) %>% filter(medianRatingTime > 0)

# combine both sets of timing data
timing_data = rbind(control_timing_data,primed_timing_data)

participants_to_exclude_based_on_timing <- timing_data %>% filter (medianRatingTime < 5.0) %>% select(ResponseId)

# combine this list of participants with
# the list of those excluded based on their rating values
participants_to_exclude = rbind(participants_to_exclude, participants_to_exclude_based_on_timing)

# then filter them all out.
processed_data <- long_responses_grouped %>% filter(!(ResponseId %in% participants_to_exclude$ResponseId))

```

## Confirmatory Analysis

As described in the planned analysis section we fit a GLMM to the participants trustworthiness ratings (see below). 

<!--We found a significant Profile Type x Primed interaction term, with the following effect size (Beta = XXX, SE = XXX, p < XXX).-->

I obtained eight pilot responses. Three were from other people (I posted a link to my Qualtrics survey on Facebook) and five were from myself. None of the other people reported issues, but I noticed a few problems that I fixed (missing question text, inconsistent default slider values, etc.). One response is excluded from the analysis because of the low standard deviation of the ratings. 

Using the pilot data, we see no significant terms in the GLM model. When I entered my pilot responses, I usually consciously tried to put in data that would match the hypothesis. While the model doesn't show significant results, it does have coefficients with the magnitudes and signs that I expected (e.g. AI x Condition interaction term has a negative Beta coefficient).

[*Single test and justification*: If we find that the AI x Condition interaction is significant and has a negative Beta coefficient, then we consider the experiment to have successfully been replicated - since this indicates that AI-like profiles were rated less trustworthy in the treatment condition (where participants are told some profiles are AI-generated) than in the control (where they believe all profiles to be human-generated). Otherwise, we consider the experiment to not have successfully been replicated.]

```{r}
lme(meanScore ~ ai * condition, random=~ 1|ResponseId,data=processed_data)
```

```{r, echo=FALSE}
reviews <- tibble("AI Score"=c("5 (most AI-like)", "1.75 (most human-like)"), "Profile Text"=c("Hey, I'm an health nut and entrepreneur. Enjoy cooking, reading, writing. Favorite restaurant nearby: Momofuku Ssam bar. Favorite weird activity nearby: Russian Baths on E. 10th St. Heaven is three grocery stores and a farmer's market within two blocks (Trader Joe's, Whole Foods, Westside Market, farmer's market", "I am a musician that travels with my band a lot. I have a place in Hawaii as well that I spend time in. I'm clean. funky. and a fun artist. Love yoga, books, writing, singing, dancing, dj'ing, art, and the list goes onnn. "))
#knitr::kable(reviews,caption="Table 1: The top 'human-like' and 'AI-like' sample profiles used in Experiment 3.") %>%  column_spec(1, width="12em")
```

###Exploratory analyses
**TODO**

## Discussion
**TODO**

### Summary of Replication Attempt
**TODO**

### Commentary
**TODO**

### Footnotes {#footnotes}

<!--
###Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.

###Planned Sample

Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.

###Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

###Procedure	

Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

###Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

###Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


##Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
###Data Preparation

####Load Relevant Libraries and Functions

####Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

###Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
-->