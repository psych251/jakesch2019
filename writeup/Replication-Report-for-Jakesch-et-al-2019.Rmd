---
title: "Replication of Experiment 3 by Jakesch et. al. (2019, ACM CHI Conference on Human Factors in Computing Systems)"
author: "Jacob Ritchie (jritchie@stanford.edu)"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: replication.bib
output:
  html_document:
    fig_crop: no
    fig_width: 9
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
    
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

In the ACM CHI 2019 paper "AI-Mediated Communication: How the Perception that Profile Text was Written by AI Affects Trustworthiness"  [@Jakesch2019], the authors conducted a Wizard-of-Oz crowdworker study to test the hypothesis that users would react to text from an online marketplace listing differently when they believed this text had been generated automatically using "artificial intelligence" (AI) ^[Otherwise known as machine learning or statistical function approximation.]. In particular, they investigated whether the belief that an AirBnB host profile was written by AI would decrease the perceived trustworthiness of the host described in the profile. They found evidence for what they called the "Replicant Effect": that the belief that a profile was written by AI decreased perceived trustworthiness, but *only* for the case where participants believed that some, but not all profiles were written by AI.
 
In this replication, we focus on independently confirming one part of the Replicant Effect - that when presented with a mixed set containing both "human-generated" and "AI-generated" profiles, participants will assign lower trustworthiness scores to those profiles that seem the most "AI-like". We do this by replicating Experiment 3 from the original paper, which found that "AI-like" profiles were given significantly lower trustworthiness scores by participants in three treatment conditions where they were told that a subset of the profiles they were rating were AI-generated^[In reality, all of the profiles were human-generated.]. We focus on the "primed" treatment condition of Experiment 3, which resulted in the largest observed effect size. In this treatment, participants are asked to rate how likely they believed it was that each profile was AI-generated before they assigned that profile a trustworthiness rating.

We leave to future work a replication of the other half of the Replicant Effect - that when participants are presented with a sample of profiles and told that all the profiles are AI-generated, then they assign the same trustworthiness ratings as when they believe that all the profiles are human-written.

### Links

See https://osf.io/z2bgj for a link to our preregistration.

See http://rpubs.com/jritch/jakesch2019_proposal for the original project proposal.

See http://rpubs.com/jritch/jakesch2019 for the most recent project report.

See http://github.com/psych251/jakesch2019 for the complete GitHub project.

See https://stanforduniversity.qualtrics.com/jfe/form/SV_a4WTJ0FvjqS5vJX for a link to the experimental setup on Qualtrics.

## Methods

### Power Analysis

The original paper reports a large effect size in Experiment 3 (total R^2^ = 0.48), so we anticipated that a power analysis would show that smaller sample is needed than in the original study.

In terms of feasibility, the original experiment required 323 participants working for six minutes each, resulting in 32.3 person-hours of work in total, which is not feasible given the budget available for the project. This is why this replication focuses on only one of the three treatment conditions.

Using the 'simr' package, we determined that using the original sample size (108 total participants after exclusions, across the two conditions we planned to focus on - control and primed) would provide 89.70% power (95% CI: [87.65, 91.51]) for the fixed effect of interest (the AI x Condition interaction term in the linear mixed-effects model), assuming participants are excluded at similar rates to the original study.

To ensure approximately 80% power, we would require 77 total participants (this gives 80.05% power, 95% CI: [78.78, 81.28]). To ensure approximately 95% power, we would require 130 total participants (this gives 95.03% power, 95% CI: [94.31, 95.68]). 

Power analysis code is located in the github repository in the power_analysis.R file, and results of a power curve analysis are reported in power_curve.txt


### Planned Sample

Based on the power analysis, we planned to recruit 170 participants (85 participants per condition) slightly above the number in the original sample, which would give above 90% power to retrieve the effect of interest assuming that we would exclude participants at a similar rate.

As in the original paper, we recruited participants from Amazon Mechanical Turk. We used the participation criteria from the original paper:

> "Participation was limited to adults in the US who had completed at least 500 tasks with an approval rate of >= 98%."

### Materials

We reproduced the materials described in the original paper, which were mostly available from the associated OSF repository: https://osf.io/qg3m2/

To explain the Wizard-of-Oz AI system to the participants in the treatment condition, we used the explanation and demo video from the original experiment:

>"In the final design, we explained the AI system as follows: 'To help hosts create profiles that are more attractive, this site provides a computer system using artificial intelligence that will write the description for hosts. The hosts simply enter some information and the artificial intelligence system generates the profile.' The participants in the treatment group then watched a 10-second demo video of a mock-up AI system... In the video, an animation depicts a system automatically generating text for an Airbnb profile from a Facebook profile"

We used the three-item trustworthiness rating scale from the original experiment (each item ranged from (0%) "Not Confident" to (100%) "Highly Confident", in increments of 10):

>"1. This person maintains a clean, safe, and comfortable household."

>"2. This person will be concerned about satisfying my needs during the stay."

>"3. This person will not intentionally harm, overcharge, or scam me."

We also used the generalized trust and AI attitude scales from the original experiment. In the treatment condition, we used the AI score measurement described in the original study:

> '...respondents indicated whether they thought each profile was (1) "Definitely Human-written" to (6) "Definitely AI-generated" on a 6-point Likert-style scale. We refer to this measurement as the "AI score" of a profile." '

We used the original set of 30 sample profiles from the original paper's Experiment 3, which we downloaded from the project's OSF repository. These had been determined in a previous mTurk task to be the 15 most "AI-like" and 15 most "human-like" from a larger sample of profiles, using the AI score measurement described above (see Table 1 for examples).

A variety of other questions were also asked, as in the original study:

> "Participants also answered demographic questions (gender, age, education, and residential neighborhood type), as well as free-form questions explaining how they rated the profiles. We finally asked what they thought was the purpose of this study, and, in the treatment group, whether they had comments on the system."

We contacted the authors to request a copy of the exact questions and instructions given to the participants (which were not all initially included in the authors' publically available OSF repository), including the video prompt. This ensured that the same questions were used, and that the framing and context of each question was as close as possible to the original experiment. The original authors were very responsive and promptly made the requested materials available.


```{r, include=FALSE, message=F, warning=F}
#path = sub("[^/]*.Rmd", "", rstudioapi::getActiveDocumentContext()$path)
#setwd(path)

library(Rmisc)
library(Hmisc)
library(stats)
library(nlme)
library(dplyr)
library(kableExtra)
library(tidyr)
library(stringr) # useful for some string manipulation
library(ggplot2)
library(plotrix)

```


```{r, echo=FALSE}
reviews <- tibble("AI Score"=c("5 (most AI-like)", "1.75 (most human-like)"), "Profile Text"=c("Hey, I'm an health nut and entrepreneur. Enjoy cooking, reading, writing. Favorite restaurant nearby: Momofuku Ssam bar. Favorite weird activity nearby: Russian Baths on E. 10th St. Heaven is three grocery stores and a farmer's market within two blocks (Trader Joe's, Whole Foods, Westside Market, farmer's market", "I am a musician that travels with my band a lot. I have a place in Hawaii as well that I spend time in. I'm clean. funky. and a fun artist. Love yoga, books, writing, singing, dancing, dj'ing, art, and the list goes onnn. "))
knitr::kable(reviews,caption="Table 1: The top 'human-like' and 'AI-like' sample profiles used in Experiment 3.") %>%  column_spec(1, width="12em")
```


### Procedure

We followed the procedure from the original paper exactly, with the exception that we only had one treatment group rather than three:

>"We randomly assigned participants to one of four groups [*note: two in the replication study*]: The control group participants were told they were rating regular profiles written by the host (akin to the "host-written" group in Study 1). In the treatment groups, participants were told that "some of the profiles [they] see have been generated by a computer system using artificial intelligence, while others have been written by the host." Treatment group participants also viewed the [demo video]."

> "We performed a manipulation check to verify that the participants in the treatment condition understood that a profile had been generated."

After the introduction to the experiment, participants then completed the trustworthiness rating questions (see Materials):  

> "Participants rated 10 profiles in randomized order: five 'AI' profiles (out of the 15 profiles rated as AI-like in the preliminary selection) and five 'human' profiles (out of the 15 profiles rated human-like)..."

> "Subjects in the primed condition were not shown any labels, but we asked them... to rate the AI score of a profile before they rated the host's trustworthiness."

After they rated all ten profiles, participants answered the generalized trust and AI attitude scales, and answered the demographic and open-ended questions from the original study.

### Planned Analysis 

We followed the exclusion criteria from the original paper:

> "We excluded responses that had failed the linguistic attentiveness check borrowed from Munro et al. [43] as well as participants who did not select the right scenario ("I am traveling and the person in the profile offers to host me.") in a second attentiveness test. We excluded workers whose median rating time per profile was less than five seconds and workers with mostly uniform responses (SD < 5.0). Furthermore, we removed participants whose average trust rating fell outside the mean +/- 2 SD statistic of participant rating averages, leaving us with 389 subjects."

The mean of the three items in the trustworthiness scale was averaged to create an overall trustworthiness rating. 

Then, we fit a linear mixed-effects model (LME) with random per-participant effects to each participants' trustworthiness ratings: `lme(meanScore ~ ai * condition, random=~ 1|ResponseId)`.

We chose the interaction term from this model as the key statistic of interest. If we found that the AI x Condition interaction in this model is significant and has a negative Beta coefficient, then we would consider the experiment to have been successfully replicated - since this indicates that AI-like profiles were rated less trustworthy in the treatment condition (where participants are told some profiles are AI-generated) than in the control (where they believe all profiles to be human-generated). Otherwise, we would consider the experiment to not have successfully been replicated.

### Differences from original study

We used a lower rate of compensation than the original study (7.25 USD/hour in the replication compared to 12 USD/hour in the original study), which may have resulted in quicker and less thoroughly considered answers if the crowd workers compensated for the lower rate of pay by completing the task more quickly. However, we don't expect that this significantly impacted the results.

Because of the limited resources available for the replication, we only replicated one of the three treatment conditions ("labeled", "unlabeled" and "primed') from the original paper (the "primed" condition). However, since the three treatment conditions produced similar results in the original study, and the 'primed' treatment condition resulted in the largest effect size, we believe that testing the replicability of this condition is sufficient.

### Methods Addendum (Post Data Collection)

#### Actual Sample
We recruited 170 participants, as planned. After exclusions we had 110 participants, similar to the 108 post-exclusion participants in the original study's primed and control conditions.

#### Differences from pre-data collection methods plan
There were no significant differences between the pre-registered methods and confirmatory analysis and the analysis performed.

One aspect of our survey implementation which may have affected the responses was a slight delay in loading the profile text, due to our javascript profile randomization. This may have decreased the accuracy of the responses if it led participants to start answering the questionnaire before fully reading the profile.

### Results

## Data Preparation

We excluded responses that met the exclusion criteria described in the original paper. 

For each profile, the mean of the three items in the trustworthiness scale was calculated and used as an overall trustworthiness rating, as done in the original study.


```{r}
responses <- read.csv(file="jakesch_et_al_2019_replication_survey_data_anonymized.csv") %>% 
  # filter out the Qualtrics header info lines
  # select only the columns needed for the confirmatory analysis
  filter(!between(row_number(),1,2)) %>% 
   # filter out participants who did not consent 
   # or who revoked consent after the debrief
   # or who failed one of the two attention check questions
       filter(
       Consent == 'I CONSENT to take part in the study.',
       debrief_agree == "I AGREE that the data collected during the study may be included for the purpose of the study.",
       att_check_1 == "John is easy to see.",
       att_check_2 == "I am traveling and the person in the profile offers to host me." #,
       #(ai_comprehension == "Some were generated by the AI system, some were written by the host." | ai_comprehension == "")
       # The original paper notes that they did not exclude participants based on the AI comprehension check question in their final analysis.
   ) %>%

  select(starts_with("X"), -contains("ai_score"),  c("ResponseId","condition","question_order")) %>% 
   # separate the question order string into ten different columns
   separate(question_order,into = paste("question_order", 1:10, sep="_"),remove=TRUE )

# tidy the data
long_responses <- responses %>% select(-contains("timing")) %>%
  gather (measurement, score, -c("ResponseId", "condition"),  -contains("question_order")) %>% 
  mutate(
    score = as.numeric(str_remove_all(score,"[^0-9]")),
    # ai is TRUE if the profile is "ai-generated, 
    # and FALSE if the profile is "human-generated"
    order_num = paste("question_order_", sub("X([0-9]*)_.*","\\1",measurement),sep=""))

# Used column selection code from:
# https://stackoverflow.com/questions/25122498/r-select-value-from-a-different-column-for-each-row
# this lets us find the profile number that was shown for each question
# which in turn will let us calculate whether that trial was AI or human

question_order_colnames = sapply(seq_along(long_responses[,1]), 
                     function(x) long_responses$order_num[x])

question_order_indices = sapply(seq_along(long_responses[,1]), function(x) which(colnames(long_responses) == question_order_colnames[x]))

long_responses$profile_number <- sapply(seq_along(long_responses[,1]), 
                     function(x) long_responses[x, question_order_indices[x]])

# add ai and item_number
long_responses <- long_responses %>%
  mutate(
    # profiles 1-15 are "AI", 16-30 are "human"
    ai = as.numeric(profile_number) <= 15,
    # which of the three items on the 3-item trustworthiness scale is this? 
    item_number = as.numeric(str_sub(measurement,-1,-1))
  )

long_responses_grouped <- long_responses %>%
  group_by(ResponseId,profile_number,condition,ai) %>%
  summarize(meanScore = mean(score, na.rm = T)) %>% 
  filter(!is.na(meanScore)) %>% 
  ungroup()

# for each participant, calculate their average trustworthiness rating
# and the std. deviation of their ratings
participant_average_rating = long_responses_grouped %>% group_by(ResponseId) %>% summarize(meanRating = mean(meanScore),
          sdRating = sd(meanScore))
# calculate the mean std. deviation and mean 
# of ALL per-participant average trustworthiness ratings
rating_mean = mean(participant_average_rating$meanRating)
rating_sd = sd(participant_average_rating$meanRating)

# get list of participants to exclude
# based on either overly uniform scores, 
# or on an extreme average score
participants_to_exclude = participant_average_rating %>% filter((meanRating > rating_mean + 2 * rating_sd) | meanRating < (rating_mean - 2 * rating_sd) | (sdRating < 5.0) ) %>% select(ResponseId)

# we only care about the "Page.Submit"
# value of each rating question page
timing_data <- responses %>% gather(time_interval, time, contains("timing_"), -contains("Click")) %>% select(time_interval,time,ResponseId) %>% group_by(ResponseId) %>% summarize(medianRatingTime = median(as.numeric(time),na.rm=T)) %>% filter(medianRatingTime > 0)

participants_to_exclude_based_on_timing <- timing_data %>% filter (medianRatingTime < 5.0) %>% select(ResponseId)

# combine this list of participants with
# the list of those excluded based on their rating values
participants_to_exclude = rbind(participants_to_exclude, participants_to_exclude_based_on_timing)

# then filter them all out.
processed_data <- long_responses_grouped %>% filter(!(ResponseId %in% participants_to_exclude$ResponseId))

```

## Confirmatory Analysis

As described in the planned analysis section we fit an LME to the participants trustworthiness ratings (see below). 


```{r}
model = lme(meanScore ~ ai * condition, random=~1|ResponseId, data = processed_data)
summary(model)
anova(model)
```

## Exploratory analyses

In the exploratory analysis, we investigate the other model reported in the original paper, visualize the results of the main confirmatory analysis, reproduce the two relevant figures from the original paper, perform a small fixed-effect meta-analysis to pool the information from the original and replication studies, and do some exploratory data analysis and visualization to investigate possible profile-level differences.

```{r, message=F, warning=F} 

library(cowplot)
library(meta)
library(gridExtra)
```

Our confirmatory analysis was based on Model 2 from [@Jakesch2019]. We can repeat this analysis with a linear model (no per-participant random effects), which corresponds to that paper's Model 1.
```{r}
model_lm = lm(meanScore ~ ai * condition, data = processed_data)
summary(model_lm)
anova(model_lm)
```


Load original data for side-by-side visual comparison 
```{r}
load("original_data.RData")

original_processed_data <- original_processed_data %>% 
  rename(ai = type, meanScore = rating, profile_number = "profile" ) %>%
  mutate(condition = factor(condition, levels = c("control","unlabeled","labeled","primed")))
  
original_plot_d31 <- original_plot_d31 %>% rename(ai = type, meanScore = rating) 
original_plot_d32 <- original_plot_d32 %>% rename(ai_score = aiscore, 
                                                  ai_score_r = aiscore_r, 
                                                  meanScore = rating)
```

Revalue the columns in the data frame to make plots more readable, and calculate the normalized score for each participant.
```{r}
processed_data <- processed_data %>% mutate(ai = ifelse(ai==TRUE,"ai","human"))
processed_data <- processed_data %>% mutate(ai = factor(ai, levels=c("human", "ai")))
processed_data <- processed_data %>% mutate(profile_number = as.numeric(profile_number))

processed_data  <- processed_data %>% group_by(ResponseId) %>% mutate(meanRating = mean(meanScore)) %>% ungroup %>% mutate(normalized_score = meanScore - meanRating) 
```

For each figure, we will define a function that generates that plot, and call that function on both the original data and the data from the replication. The resulting plots are displayed as small multiples sharing a common vertical scale, to facilitate comparison.

The first plot shows the data divided by condition (control vs. the three treatment effects) and profile type (Human vs. AI)^[In the initial analysis I coded this variable as AI (with two values - True and False). For readibility I will revert to the variable name used in the original study, Profile Type (with two levels - "Human" and "AI").]. Each individual data point is displayed, with jittered horizontal positions.

```{r fig.cap="**Figure 1: Mean trustworthiness differences across condition and profile type (error bars are 95% CIs).**"}
plot_data <- function (data, title) {
  
  ggplot(data, aes(x=ai,y=meanScore)) + 
    geom_point(position="jitter",color="lightpink",alpha=0.8) + 
    stat_summary(fun.y=mean, geom="point",color="hotpink4",size=3) + 
    stat_summary(fun.data=mean_cl_boot, geom="errorbar",color="hotpink4",width=0,size=1) + 
    theme_cowplot() + facet_grid(~condition) + ggtitle(title) + xlab('Profile Type (Human vs. AI)') + 
    ylab('Mean trustworthiness') + scale_x_discrete(labels = c('"Human"','"AI"')) 
}

# plot both original and replication data
p1 <- plot_data(original_processed_data, "Original")
p2 <- plot_data(processed_data, "Replication")
plot_grid(p1, p2, align = "h", ncol = 2, rel_widths = c(0.65, 0.35) )

```

We can re-plot this data to match the original figures from Experiment 3 of [@Jakesch2019]^[The code for these plots is adapted from the original authors' Analysis.R script. A modified version of the Analysis.R script, where I save the output data of interest in original_data.RData, is available in the ../original_paper directory.].

Note that the original graphs use a truncated axis to allow readers to more clearly see the effect of interest. They also calculate 95% confidence intervals using the normalized scores for each participant, which better reflects the assumptions of the linear mixed-effects model used in the confirmatory analysis.  

```{r fig.cap="**Figure 2: Mean trustworthiness differences across condition and profile type (error bars are 95% CIs).**"}

plot_d31 <- summarySE(processed_data, 'meanScore', groupvars = c('ai','condition'))
plot_d31_within <- summarySE(processed_data, 'normalized_score', groupvars = c('ai','condition'))
plot_d31 <- merge(plot_d31[,c("ai","condition","N","meanScore")],plot_d31_within[,c("ai","condition","sd","se","ci")],by=c("ai","condition")) 

plot_figure_1 <-function (data,title) {

  pd <- position_dodge(0.25)
  sh <- c(19, 15)
  colors = c('dodgerblue2', 'gray20')

  ggplot(data, aes(x=condition, y=meanScore, colour=ai, group=ai)) + #geom_line(position=pd) + 
    geom_errorbar(aes(ymin=meanScore-ci, ymax=meanScore+ci), colour="black", width=.2, position=pd)+
    geom_point(position=pd, size  = 4) + 
    theme_bw(base_size = 15) + xlab('Condition') + 
    ylab('Mean trustworthiness') + 
    theme(legend.position="top", legend.box = "horizontal", 
          legend.box.margin=margin(0,0,-10,0), legend.background=element_blank(),
          axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)), 
          axis.title.x = element_text(margin = margin(t = 10, r = 20, b = 0, l = 0))) +
    labs(color='Profile type') +  
    ggtitle(title) +
    scale_color_manual(values=colors, labels = c('"Human"','"AI"')) + 
    ylim(60,75)
}

p1 <- plot_figure_1(original_plot_d31, "Original")
p2 <- plot_figure_1(plot_d31, "Replication") + scale_x_discrete(labels = c("Control","Primed"))

plot_grid(p1, p2, align = "h", ncol = 2, rel_widths = c(0.60, 0.40))

```


The following code replicates the second figure for Experiment 3, which shows the association between AI scores and trustworthiness scores.

Associating the profile number with the AI score requires us to reload and repeat some cleaning of the survey data.
```{r}
pd = position_dodge(0.6)
# read in the data
# get the answer to the ai score question
# and repeat some of the data cleaning steps 
# from above to get the profile number associated with each ai_score

ai_scores = read.csv(file="jakesch_et_al_2019_replication_survey_data_anonymized.csv") %>%
  filter(!between(row_number(),1,2)) %>% 
  select(contains("ai_score"),c("ResponseId",question_order)) %>% 
  gather(score_name,ai_score,contains("ai_score")) %>% 
  mutate(ai_score = as.numeric(str_remove_all(ai_score,"[^0-9]")),
         score_number = as.numeric(str_remove_all(score_name,"[^0-9]"))) %>% 
  filter(!is.na(ai_score)) %>% 
  separate(question_order,into = paste("", 1:10, sep=""),remove=TRUE ) 

question_order_colnames = sapply(seq_along(ai_scores[,1]), 
                     function(x) ai_scores$score_number[x])

question_order_indices = sapply(seq_along(ai_scores[,1]), function(x) which(colnames(ai_scores) == question_order_colnames[x]))

ai_scores$profile_number <- sapply(seq_along(ai_scores[,1]), 
                     function(x) ai_scores[x, question_order_indices[x]])

# drop all the intermediate columns
ai_scores <- ai_scores %>% select(ResponseId,profile_number,ai_score) %>% mutate(profile_number = as.numeric(profile_number))

# join the ai_scores with the long_responses_grouped data
# then normalize the scores for each question by subtracting 
# the participants mean rating across all 10 questions
# which is stored in participant_average_rating
# because this is an inner join, it drops all control participants (who don't have ai_scores)

normalized_responses_with_ai_scores = inner_join(processed_data,ai_scores,by=c("ResponseId","profile_number")) 

# following the analysis from the original paper, we calculate the 95 CIs% based on the normalized responses

plot_d32 <- summarySE(normalized_responses_with_ai_scores, 'meanScore', groupvars = c('ai_score'))
plot_d32_within <- summarySE(normalized_responses_with_ai_scores, 'normalized_score', groupvars = c('ai_score'))
plot_d32 <- merge(plot_d32[,c("N","meanScore","ai_score")],plot_d32_within[,c("ai_score","sd","se","ci")],by=c("ai_score")) 

```

Now that we have the AI score data in the same format as the data from the original study, we can plot them and compare.
```{r fig.cap="**Figure 3: Mean trustworthiness vs. AI score (primed condition only, error bars are 95% CIs).**"}
plot_d32$ai_score_r <- revalue(factor(plot_d32$ai_score), c("1"="1\nDefinitely\nhuman-written", "2"="2", "3"="3", "4"="4", "5"="5", "6"="6\nDefinitely\nAI-generated"))

plot_trust_curve <- function(data,title) {
  colors <- c('dodgerblue2', 'gray20')
  plot <- ggplot(data, aes(x=ai_score_r, y=meanScore)) +
    geom_errorbar(aes(ymin=meanScore-ci, ymax=meanScore+ci), colour="black", width=.2, position=pd) +
    geom_line(aes(x = ai_score, color = ai_score)) +
    geom_point(aes(x = ai_score, color = ai_score), shape = 19, size  = 4) + 
    theme_bw(base_size = 15) + xlab('AI score') + 
    ylim(50,82) +
    ylab('Mean trustworthiness') + 
    theme(axis.title.y = element_text(margin = margin(t = 0, r = 10, b = 0, l = 0)), 
          axis.title.x = element_text(margin = margin(t = 0, r = 20, b = 0, l = 0))) +
    guides(colour=FALSE) +
    ggtitle(title) + 
    scale_colour_gradientn(colours = colorRampPalette(colors)(100), limits=c(1, 6), breaks=c(1.2, 2.73, 4.266, 6))
    
  return(plot)
}


p1 <- plot_trust_curve(original_plot_d32, "Original")
p2 <- plot_trust_curve(plot_d32, "Replication")

plot_grid(p1, p2, align = "h", ncol = 2, rel_widths = c(1/2, 1/2))
```


Considering the results observed so far, we then performed a small fixed-effect meta-analysis with the results of the key statistical test of interest from the original study and our replication.

Because this is a direct replication, a fixed-effect meta-analysis is most appropriate; because the two studies were effectively identical, we assume that there is one true effect that is measured in both studies, and that all observed variation is due to sampling error [@Borenstein2010].

```{r fig.cap="**Figure 4: Fixed-effect meta-analysis forest plot.**", fig.height=2}
# From ../original_paper
original_effect_size = -4.976
original_SE = 1.464

replication_effect_size = summary(model)$tTable[4,1]
replication_SE = summary(model)$tTable[4,2]

# effect sizes
es = c(original_effect_size,replication_effect_size)
# standard error
SE = c(original_SE,replication_SE)

meta_analysis = metagen(es,SE,comb.random = FALSE)
forest.meta(meta_analysis)

```

We see that when we pool the observations from both studies, there is still aggregate evidence that the Replicant Effect (operationalized as the AI x Condition interaction term in the mixed-effects model) holds in this scenario.

We also see that the point estimate for the coefficient of interest obtained in our replication study lies within the 95% CI calculated by metagen for the effect observed in the original study. 

Finally, we will do some exploratory analysis to investigate the different effects of the 30 profiles used in the study.

```{r fig.cap="**Figure 5: Profile-level variation in treatment effect.**", fig.height=9}
slope_plot <- function(data,title) {
  colors = c('dodgerblue2', 'gray20')
  ggplot(data, aes(x = condition, y = meanScore, color = ai)) +
    geom_jitter(color = "grey",alpha = 0.8,width = .2) +
    stat_summary(fun.y = "mean", geom = "line",size = 1, aes(group = 1)) +
    stat_summary(fun.y = mean, geom = "point", size = 2.5) +
    ylim(0, 100) +
    facet_wrap( ~profile_number, ncol = 15) +
    theme_cowplot() +
    theme(axis.text.x = element_text(angle = 45,  hjust = 1,size=8)) +
    ylab('Mean trustworthiness')+
    xlab('Condition')+
    ggtitle(title) +
    scale_x_discrete(labels=c("Control","Primed")) +
    scale_color_manual(values=colors, labels = c('"Human"','"AI"')) + labs(color='Profile type')
}

p1 <- slope_plot(original_processed_data %>% filter(condition %in% c("control","primed")),"Original")
p2 <- slope_plot(processed_data,"Replication")
plot_grid(p1, p2, align = "v", nrow = 2)

```
It appears that there is significant variation in the effect of condition on score for the different profiles within each profile type. We also note that in the replication we observe lower mean trustworthiness for a much smaller number of the "AI" profiles compared to the original experiment. 

## Discussion

### Summary of Replication Attempt
Based on the pre-registered key statistic of interest, we must consider this replication to either have failed or, at best, to represent a partial replication. 

While we still see a difference in the raw data between mean trustworthiness ratings for the "Human" and "AI" profiles in the primed condition (67.9 vs. 64.4), the AI x Condition interaction term in the linear mixed-effects model is not significant (Beta = -2.638, SE = 1.748, p = 0.13). The relevant coefficient is in the same direction as the coefficient in the original paper (Beta = -4.976, SE = 1.464, p < 0.001 ), but smaller in magnitude.

One contributing factor is that the observed trustworthiness values in the control condition are significantly lower than those observed in the original paper (see Figures 1 and 2). The difference is significant enough that it seems unlikely to be purely due to sampling error, since the values in the replication fall well outside the 95% confidence intervals from the original study. It is possible that this reflects a change in opinion about online roomsharing services such as AirBnB over the time interval between the original and replication studies.

When we analyze a linear model without random effects (equivalent to Model 1 in the original paper), the key statistic of interest remains non-significant, but we do observe a marginally significant positive effect of primed condition on trustworthiness (Beta = 3.5608, SE = 1.5932, p = 0.04686). This indicates that, without normalizing away the differences between participants, the AI-mediated communication framing appeared to actually increase reported trust in hosts, which is counterintuitive given the findings of the original study.

### Commentary
While it failed to meet our pre-registered criteria, this results of this study and the meta-analysis (Figure 4) suggest that it is likely the Replicant Effect does exist, though it is probably smaller in magnitude than the what was reported in the original study.

The existence of the effect is supported by the observed inverse relationship between AI score and trustworthiness rating (see Figure 3). This provides strong evidence that participants gave lower trustworthiness scores to profiles they suspected more strongly to be AI-generated. However, unlike the original study, we did not observe any floor effect, instead observing a linear relationship between AI score and perceived trustworthiness.

The results observed in the original study may represent an example of the "Winner's Curse": the tendency for most newly discovered effects to be inflated estimates of the true effect size, by virtue of the fact that the results were noteworthy enough to be published in a selective research venue [@Young2008]. This can affect even a well-designed and pre-registered study such as the one replicated here. 

Our replication also demonstrates the related danger of relying on post-hoc power estimates. Though we calculated that this study would have 90% power to retrieve the effect of interest, this was based on the (possibly inflated) effect size estimate from the original paper, and the true power was likely much lower.

To confirm whether the failure to replicate was due to sampling error and a lack of statistical power rather than another factor (such as shifts in public opinion on AI or roomsharing services), it would be useful to perform a second replication at 90% power, based off the updated estimate of effect size calculated in the meta-analysis. 

The significant profile-level variation observed in the exploratory data analysis (Figure 5) indicates that there may be a need for a more complex statistical model that captures item-specific effects. 

Finally, we should also consider the other two experimental conditions ("unlabeled" and "labeled") from the original experiment. These conditions were more ecologically valid (and therefore arguably more interesting), but resulted in smaller effects on trustworthiness ratings than the primed condition. The new results suggest that a replication of these two conditions should also be considered.


### References and Footnotes {#footnotes}

<!--
###Power Analysis

Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size.  Considerations of feasibility for selecting planned sample size.

###Planned Sample

Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any.

###Materials

All materials - can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

###Procedure	

Can quote directly from original article - just put the text in quotations and note that this was followed precisely.  Or, quote directly and just point out exceptions to what was described in the original article.

###Analysis Plan

Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible.  

**Clarify key analysis of interest here**  You can also pre-specify additional analyses you plan to do.

###Differences from Original Study

Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect.

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or none.


##Results


### Data preparation

Data preparation following the analysis plan.
	
```{r include=F}
###Data Preparation

####Load Relevant Libraries and Functions

####Import data

#### Data exclusion / filtering

#### Prepare data for analysis - create columns etc.
```

### Confirmatory analysis

The analyses as specified in the analysis plan.  

*Side-by-side graph with original graph is ideal here*

###Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
-->